{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "248e2a1c",
   "metadata": {},
   "source": [
    "   # Project: Automatic classification of skin lesions (melanoma detection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1937b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "#Feature Selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "#Pearson Correlation\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "#Normalizing data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "#Roc CUrve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#SVMs\n",
    "from sklearn import svm\n",
    "\n",
    "#kNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Evaluation\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afd6c43",
   "metadata": {},
   "source": [
    "## I Database :\n",
    "\n",
    "Among the given documents in campus, a folder containing three types of picture was available . This folder, named PROJECT_Data, contains :\n",
    "\n",
    "-.The pictures of the skin lesions\n",
    "-.The pictures of the skin lesion segmentation\n",
    "-.The associated superpixel image of the skin lesion\n",
    "\n",
    "Before extracting the features of each image, it is important to separate these three types of images into separate lists, so we can manipulate them more easily after. \n",
    "\n",
    "As the pictures of the skin lesions are the only one in jpg format, they are easy to isolate.\n",
    "The associated segmented and superpixel pictures are in consecutive order, so I have stored them in a list and then split them into two lists by taking the even indexes for segmented images and odd indexes for superpixels indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073d85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = r\"C:\\Users\\Smail\\Documents\\TB3 Image and Pattern Recognition\\Skin lesion project/PROJECT_Data/*.jpg\"\n",
    "img_dir2 = r\"C:\\Users\\Smail\\Documents\\TB3 Image and Pattern Recognition\\Skin lesion project/PROJECT_Data/*.png\"\n",
    "\n",
    "images_originales = [] #Skin lesion pictures\n",
    "autres_images = [] # Other pictures ( intermediate variable )\n",
    "images_segmentation = [] #Segmented skin lesions \n",
    "images_superpixel = [] # Superpixels associated to the skin lesions\n",
    "\n",
    "files = glob.glob(img_dir)\n",
    "files2 = glob.glob(img_dir2)\n",
    "\n",
    "for file in files:\n",
    "    img = cv2.imread(file)\n",
    "    images_originales.append(img)\n",
    "\n",
    "for file in files2:\n",
    "    img = cv2.imread(file)\n",
    "    autres_images.append(img)\n",
    "    \n",
    "n = len(autres_images)\n",
    "for i in range(0,n,2):\n",
    "    images_segmentation.append(autres_images[i])\n",
    "\n",
    "for i in range(1,n,2):\n",
    "    images_superpixel.append(autres_images[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7c5cb",
   "metadata": {},
   "source": [
    "Let's verify that our lists contains the correct elements :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images_originales[0])\n",
    "plt.show()\n",
    "plt.imshow(images_segmentation[0])\n",
    "plt.show()\n",
    "plt.imshow(images_superpixel[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4c42c",
   "metadata": {},
   "source": [
    "We can see that the segmented picture goes along with the original picture\n",
    "\n",
    "Let's now import the dataframe that contains the labels associated to each picture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e0404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Import the dataframe with the labels ####\n",
    "\n",
    "data = pd.read_csv(r'ISIC-2017_Data_GroundTruth_Classification.csv')\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1448290c",
   "metadata": {},
   "source": [
    "# 2.Feature Extraction\n",
    "\n",
    "## 1. Morphological Features\n",
    "\n",
    "As suggested by the instructions given, let's extract features from the different pictures we have in order to have some predictors for our machine learning model. \n",
    "\n",
    "We can extract morphological features from the segmented pictures by using the function *regionprops* in the package *skimage.measure*. Thanks to this function, we have access to 10 morphological features associated to each skin lesion\n",
    "\n",
    "The function **get_shape_feature** extracts the morphological features of a single picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the features from each image\n",
    "\n",
    "from skimage.measure import label, regionprops\n",
    "\n",
    "#Morphological features\n",
    "\n",
    "def get_shape_feature(image):\n",
    "    \n",
    "    \n",
    "    shape_features = []\n",
    "    \n",
    "    #Extract the region of interest\n",
    "    \n",
    "    image_region = label(image)\n",
    "    \n",
    "    #Extract the shape features\n",
    "    \n",
    "    props = regionprops(image_region)\n",
    "    \n",
    "    shape_features.append(props[0].area)\n",
    "    shape_features.append(props[0].eccentricity)\n",
    "    shape_features.append(props[0].perimeter)\n",
    "    shape_features.append(props[0].equivalent_diameter)\n",
    "    shape_features.append(props[0].extent)\n",
    "    shape_features.append(props[0].filled_area)\n",
    "    shape_features.append(props[0].minor_axis_length)\n",
    "    shape_features.append(props[0].major_axis_length)\n",
    "    shape_features.append(props[0].major_axis_length / props[0].minor_axis_length)\n",
    "    shape_features.append(props[0].solidity)\n",
    "    \n",
    "    return np.array(shape_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fff75b9",
   "metadata": {},
   "source": [
    "For some reason, my segmented pictures have 3 dimensions, so I have converted them to grayscale images using the function \n",
    "**rgb2gray**\n",
    "\n",
    "After that, I use the function **all_features** which gathers the features of our 200 pictures into one single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(image_liste):\n",
    "    \n",
    "    n = len(image_liste)\n",
    "    for i in range(n):\n",
    "        image_liste[i] = cv2.cvtColor(image_liste[i],cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "    return image_liste\n",
    "\n",
    "images_segmentation = rgb2gray(images_segmentation)\n",
    "\n",
    "\n",
    "def all_features(image_liste):\n",
    "    \n",
    "    dataframe = np.empty(shape=(200,10))\n",
    "    \n",
    "    n = len(image_liste)\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        dataframe[i,:] = get_shape_feature(image_liste[i])\n",
    "    \n",
    "    df = pd.DataFrame(dataframe, columns = ['Area','Eccentricity','Perimeter','Equivalent Diameter','Extent','Filled Area','Minor AL','Major AL','Major/Minor','Solidity'])\n",
    "\n",
    "    return df\n",
    "\n",
    "#test\n",
    "\n",
    "features = all_features(images_segmentation)\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89ca352",
   "metadata": {},
   "source": [
    "## 2. Texture features\n",
    "\n",
    "We are going to extract the texture features using the **Local Binary Patterns** on the original pictures.\n",
    "**Local Binary Patterns** is an algorithm that turns a 2D image into another image in order to detect the particular textures of this image. Thanks to the function **local_binary_pattern** of the package skimage.feature, we can apply the Local Binary Pattern algorithm to each component of our original skin lesion images. We can then plot the histogram associated to each picture and see which pixel intensity value is the most represented.\n",
    "\n",
    "Let's see if the histogram of each component is different or is the same. If it is the same, we can then use only one component and reduce the processing complexity of our code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dce4992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract descriptors with Local binary patterns\n",
    "\n",
    "method = 'uniform'\n",
    "\n",
    "image_test_red = images_originales[129][:,:,0] #red channel\n",
    "image_test_green = images_originales[129][:,:,1] #green channel\n",
    "image_test_blue = images_originales[129][:,:,2] #blue channel\n",
    "\n",
    "lbp_test_red = local_binary_pattern(image_test_red,8,1,method)\n",
    "lbp_test_green = local_binary_pattern(image_test_green,8,1,method)\n",
    "lbp_test_blue = local_binary_pattern(image_test_blue,8,1,method)\n",
    "\n",
    "hist_test_red,bins1 = np.histogram(lbp_test_red,bins = 256)\n",
    "hist_test_green,bins2 = np.histogram(lbp_test_green,bins = 256)\n",
    "hist_test_blue,bins3 = np.histogram(lbp_test_blue,bins = 256)\n",
    "\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (21, 5))\n",
    "\n",
    "ax1.plot(hist_test_red)\n",
    "ax1.set_title(\"Histogram of red component\")\n",
    "ax2.plot(hist_test_green)\n",
    "ax2.set_title(\"Histogram of green component\")\n",
    "ax3.plot(hist_test_blue)\n",
    "ax3.set_title(\"Histogram of blue component\")\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7828c1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "We see that for each channel, the histogram is the same, we are thus going to use only one channel for the processing of our skin lesion images\n",
    "\n",
    "We use the function **extract_lbp** to extract the texture features from the skin lesion images\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687f047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lbp(liste_image,nbPoints,radius):\n",
    "    \n",
    "    n = len(liste_image)\n",
    "    \n",
    "    df_features = np.empty(shape=(200,256))\n",
    "    \n",
    "    for i in range(n):\n",
    "        \n",
    "        img = liste_image[i][:,:,0] #extract red component\n",
    "        lbp = local_binary_pattern(img,nbPoints,radius)\n",
    "        hist,bins = np.histogram(lbp,bins=256)\n",
    "        \n",
    "        df_features[i,:] = hist\n",
    "        \n",
    "    df_features = pd.DataFrame(df_features, columns = ['Descriptor ' + str(i) for i in range(256)])\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "texture_features = extract_lbp(images_originales,8,1)\n",
    "\n",
    "texture_features.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716a8420",
   "metadata": {},
   "source": [
    "## 3. Label vector and Feature selection\n",
    "\n",
    "We have to import the label vector that contains the true labels associated to each picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "\n",
    "dir_images = r'C:\\Users\\Smail\\Documents\\TB3 Image and Pattern Recognition\\Skin lesion project\\PROJECT_Data'\n",
    "\n",
    "from skimage import util\n",
    "import csv\n",
    "\n",
    "for image_path in glob.glob(dir_images + \"/*.jpg\"):\n",
    "\n",
    "    #print(\"[INFO] Processing image \" + image_path)\n",
    "\n",
    "    img_ori = cv2.imread(image_path)\n",
    "    img = util.img_as_ubyte(img_ori)\n",
    "    basename=os.path.basename(image_path)\n",
    "    splitname=os.path.splitext(basename)[0]\n",
    "    \n",
    "    csv_file=csv.reader(open(\"ISIC-2017_Data_GroundTruth_Classification.csv\",\"r\"), delimiter=\",\")\n",
    "    for row in csv_file:\n",
    "        if splitname==row[0]:\n",
    "            Y=np.append(Y,int(float(row[1])))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b356367",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b742b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70623f49",
   "metadata": {},
   "source": [
    "# Morphological Feature Selection\n",
    "\n",
    "To select the most relevant morphological features, we are going to use the ANOVA criterion with the f_classif function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f9bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = SelectKBest(f_classif, k = 5).fit_transform(features, Y)\n",
    "features = pd.DataFrame(features, columns = [\"Geometrical Feature \" + str(i) for i in range(5)])\n",
    "\n",
    "features.drop(['Geometrical Feature 2'],1)\n",
    "features.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed2cedd",
   "metadata": {},
   "source": [
    "## Texture Feature Correlation\n",
    "\n",
    "To select the relevent LBP Features, we are going to use the Pearson Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5387722",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Pearson correlation\n",
    "\n",
    "#Creating the DataFrame\n",
    "pearson_corr = pd.DataFrame()\n",
    "pearson_corr[\"Descriptors\"] = texture_features.columns\n",
    "\n",
    "pearson_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e3d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = []\n",
    "\n",
    "#Computing the Pearson Correlation\n",
    "for column in texture_features.columns:\n",
    "    L.append(pearsonr(texture_features[column], Y))\n",
    "\n",
    "#Adding the p-value\n",
    "pearson_corr[\"Absolute Pearson’s correlation\"] = abs(np.array([element[0] for element in L]))\n",
    "pearson_corr[\"p-value\"] = [element[1] for element in L]\n",
    "\n",
    "#Capturing the results\n",
    "scores = pearson_corr.sort_values(by=['Absolute Pearson’s correlation'], ascending=False)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe9791",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c346dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only the important features\n",
    "selected_features = pd.DataFrame()\n",
    "for col in list(scores[\"Descriptors\"]):\n",
    "    selected_features[\"LBP \" + col] = texture_features[col]\n",
    "\n",
    "predictors = pd.concat([features, selected_features], axis = 1)\n",
    "predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd3e372",
   "metadata": {},
   "source": [
    "We now have our array of predictors ! We can normalize the data and split it into a train set and a test set.\n",
    "\n",
    "I have chosen :  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcccd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization of the data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X = min_max_scaler.fit_transform(predictors)\n",
    "X = pd.DataFrame(X, columns = predictors.columns)\n",
    "print(\"Data normalized.\")\n",
    "\n",
    "\n",
    "#Train Test Split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "print(\"Data split.\")\n",
    "print(\"Train predictor shape: \", X_train.shape)\n",
    "print(\"Test predictor shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1facec4",
   "metadata": {},
   "source": [
    "# 4. Classification using different models\n",
    "\n",
    "We are going to try a lot of classifiers and choose the best ones according to machine learning criteria such as the :\n",
    "  * Accuracy\n",
    "  * F1_Score\n",
    "  * Confusion Matrix\n",
    "  * ROC and AUC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe9302",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdcc9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Nearest_Neighbors\", \"Linear_SVM\", \"Polynomial_SVM\", \"RBF_SVM\", \"Gaussian_Process\",\n",
    "         \"Gradient_Boosting\", \"Decision_Tree\", \"Extra_Trees\", \"Random_Forest\", \"Neural_Net\", \"AdaBoost\",\n",
    "         \"Naive_Bayes\", \"SGD\", \"Logistic Regression\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025,probability = True),\n",
    "    SVC(kernel=\"poly\", degree=3, C=0.025,probability = True),\n",
    "    SVC(kernel=\"rbf\", C=1, gamma=2,probability = True),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    GradientBoostingClassifier(n_estimators=100, learning_rate=1.0),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    ExtraTreesClassifier(n_estimators=10, min_samples_split=2),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=100),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(n_estimators=100),\n",
    "    GaussianNB(),\n",
    "    SGDClassifier(loss=\"hinge\", penalty=\"l2\"),\n",
    "    LogisticRegression()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82923f89",
   "metadata": {},
   "source": [
    "# Evaluation of our models\n",
    "\n",
    "To evaluate the performance of our models , let's compute all the metrics we have talkes aboute before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f11657",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "f1_scores = []\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for name, clf in zip(names, classifiers):\n",
    "    fit = clf.fit(X_train, Y_train)\n",
    "    predictions = fit.predict(X_test)\n",
    "    score = accuracy_score(predictions, Y_test)\n",
    "    scores.append(score.mean())\n",
    "    \n",
    "for name, clf in zip(names, classifiers):\n",
    "    fit = clf.fit(X_train, Y_train)\n",
    "    predictions = fit.predict(X_test)\n",
    "    #score = clf.score(X_test, Y_test)\n",
    "    score = f1_score(Y_test,predictions)\n",
    "    f1_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['name'] = names\n",
    "df['score'] = scores\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f68a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1scores = pd.DataFrame()\n",
    "f1scores['name'] = names\n",
    "f1scores['f1 score'] = f1_scores\n",
    "f1scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e00dce",
   "metadata": {},
   "source": [
    "# Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca014d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(y=\"name\", x=\"score\", data=df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b85ddf",
   "metadata": {},
   "source": [
    "# F1 _ Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6151b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(y=\"name\", x=\"f1 score\", data=f1scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955bbfc",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a10dd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    ">>> from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "\n",
    "for names, clf in zip(names, classifiers):\n",
    "    \n",
    "    clf.fit(X_train,Y_train)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(clf,X_test,Y_test)\n",
    "    plt.title(names)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed1232",
   "metadata": {},
   "source": [
    "# ROC CURVES and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf257fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC CURVES \n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "nbclass = len(classifiers)\n",
    "\n",
    "fig, axs = plt.subplots(nbclass, 1, figsize = (25, 10))\n",
    "\n",
    "names = [\"Nearest_Neighbors\", \"Linear_SVM\", \"Polynomial_SVM\", \"RBF_SVM\", \"Gaussian_Process\",\n",
    "         \"Gradient_Boosting\", \"Decision_Tree\", \"Extra_Trees\", \"Random_Forest\", \"Neural_Net\", \"AdaBoost\",\n",
    "         \"Naive_Bayes\", \"SGD\"]\n",
    "\n",
    "\n",
    "for names , clf in zip(names, classifiers):\n",
    "    \n",
    "    clf.fit(X_train,Y_train)\n",
    "    clf_probs = clf.predict_proba(X_test)\n",
    "    clf_probs = clf_probs[:, 1]\n",
    "    clf_auc = roc_auc_score(Y_test, clf_probs)\n",
    "    clf_fpr, clf_tpr, _ = roc_curve(Y_test, clf_probs)\n",
    "    plt.plot(clf_fpr, clf_tpr, linestyle='-', label= names + '(AUROC = %0.3f)' % clf_auc)\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "\n",
    "    # Title\n",
    "    plt.title('ROC Plot ' + names)\n",
    "    # Axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # Show legend\n",
    "    plt.legend() # \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc908ae6",
   "metadata": {},
   "source": [
    "# 6 Conclusion\n",
    "\n",
    "According to the ROC curves and AUC, which is the most complete indicator of performance, the best models to detect **melanoms** are the\n",
    " * Decision Tree\n",
    " * Extra _ Trees\n",
    " * Random Forest\n",
    " \n",
    " We can reach an accuracy of 74%, which is not bad, but can be largely improved if we had more data, and more computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f7556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
